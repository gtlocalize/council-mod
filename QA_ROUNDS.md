# QA Audit Rounds - Living Document

**Purpose:** Track QA methodology, known limitations, and iterative improvements across audit rounds.

---

## Round 1 (Nov 2024) - Baseline Calibration

**Status:** In Progress  
**Goal:** Establish baseline agreement between human auditor and LLMs (Gemini 3 Pro, Claude Sonnet 4.5)

### Test Set Composition

**Total cases:** ~600 (514 original + 86 context supplement)
- **Languages:** English (67%), Japanese (33%)
- **Types:** positive (violations), negative (clean), edge (ambiguous), obfuscated, cross-category
- **Categories:** All 11 moderation categories

**Generation:**
- Model: Claude Opus 4.5
- Prompts: Category-specific with type instructions
- Context cases: Initially biased toward "context excuses", supplemented with condemning/neutral/bad-regardless patterns

### Auditors

| Auditor | Model/Human | Cases Completed | Notes |
|---------|-------------|-----------------|-------|
| Human | N/A | In progress | Deterministic shuffle (seed=42) |
| Gemini | gemini-3-pro | ~250 | Quota limited (250 RPD, Tier 1) |
| Claude | claude-sonnet-4-5-20250929 | ~600 | Complete set |

### Known Limitations & Biases

These issues were discovered during Round 1 and will be addressed in future rounds:

#### 1. Human Auditor Information Leakage (FIXED mid-round)

**Issue:** Human audit CLI initially displayed:
- Category hint
- Test case type (positive/negative/edge/etc.)
- Language
- Generation reasoning
- Case ID (with category prefix)

**Impact:** HIGH - Human could see the "answer" before making decision

**Fixed:** Removed all metadata from display mid-round (after ~20 human audits)

**Action for Round 2:** Human audits from Round 1 start may need to be redone or excluded

---

#### 2. LLM Auditor Category Hint (FIXED mid-round)

**Issue:** Gemini audit prompt included `Category hint: ${testCase.category}`

**Impact:** HIGH - Gemini wasn't truly blind, could bias toward expected category

**Fixed:** Removed category hint from prompt mid-round (after ~4 audits)

**Action for Round 2:** May need to redo Gemini audits, or compare pre/post-fix agreement rates

---

#### 3. Context Bias in Test Generation (PARTIALLY FIXED mid-round)

**Issue:** Original test cases with context (~50 cases) all followed pattern:
- "Looks bad ‚Üí context excuses it" (100% exonerating context)

**Impact:** MEDIUM - Doesn't test "context makes it worse" or "context irrelevant" scenarios

**Fixed:** Generated 86 supplemental cases with:
- 50% context condemns (makes it worse)
- 35% context neutral (irrelevant)
- 15% bad regardless

**Remaining limitation:** Original ~50 context cases still have the bias

**Action for Round 2:** 
- Flag original context cases in analysis
- Generate new balanced context set from scratch
- Add speaker attribution (see #5)

---

#### 4. Audit Order Effects (FIXED early)

**Issue:** Human audits were in deterministic order, but adding new test cases changed the shuffle

**Impact:** LOW - Could cause psychological bias (priming, fatigue patterns)

**Fixed:** 
- Implemented deterministic shuffle (seed=42) 
- Added auto-skip for already-audited cases when new cases added

**Action for Round 2:** Use same seed for consistency, or use different seed and document it

---

#### 5. Context Speaker Attribution (NOT YET IMPLEMENTED)

**Issue:** Context messages are `string[]` with no indication of who said them:
- Same user (escalation pattern)?
- Different users (group coordination)?
- Mix?

**Impact:** MEDIUM - Missing signal for stalking/harassment pattern detection

**Current workaround:** Auditors infer from context, but ambiguous

**Action for Round 2:** Implement `ContextMessage { speaker: 'same'|'other', text: string }`

---

#### 6. Context Temporal Ordering (NOT DOCUMENTED INITIALLY)

**Issue:** Unclear whether context messages are before or after the moderated content

**Impact:** LOW - Most auditors assumed "before", but not explicit

**Fixed:** Documented in PLAN.md that context = previous messages only (chronological order)

**Action for Round 2:** 
- Update generation prompts to clarify temporal order
- Ensure all context arrays are chronologically ordered (oldest ‚Üí newest)

---

#### 7. Confession vs Threat Distinction (NOT DOCUMENTED INITIALLY)

**Issue:** No clear guidance on whether past tense confessions should be flagged:
- "I killed someone" vs "I will kill you"
- "I tortured animals" vs "Here's how to torture animals"

**Impact:** MEDIUM-HIGH - Likely caused disagreements between auditors

**Fixed:** Added comprehensive framework to CATEGORY_DEFINITIONS.md:
- Confessions (past) = ALLOW (except child_safety)
- Threats (future) = DENY
- Instructions/incitement = DENY
- Distribution (sharing graphic content) = DENY

**Action for Round 2:** Re-audit confession cases with new guidelines, or exclude from agreement metrics

---

#### 8. Category Definitions Not Finalized (FIXED mid-round)

**Issue:** Fuzzy boundaries between categories:
- "I'm gonna cut your head off" - violence or threats?
- Personal expression vs platform standards
- Sexual harassment vs general harassment severity

**Impact:** HIGH - Core definitions unclear for all auditors

**Fixed:** Created comprehensive CATEGORY_DEFINITIONS.md with:
- Severity ranking
- Fuzzy boundary resolutions
- Interpersonal Safety vs Platform Content modes
- Edge case tables

**Action for Round 2:** All auditors should reference CATEGORY_DEFINITIONS.md before starting

---

#### 9. Gemini API Quota Limitations (EXTERNAL)

**Issue:** Tier 1 quota limited to 250 RPD (Requests Per Day)

**Impact:** MEDIUM - Prevented complete Gemini audit in single day

**Workaround:** Used Claude Sonnet 4.5 as second LLM auditor for faster completion

**Action for Round 2:** Consider upgrading Gemini quota tier, or continue with multi-LLM approach

---

### Decisions Made for Round 1

These were conscious choices to keep Round 1 as-is:

1. **No category severity tracking** - Deferred to Phase 6.6, tracking only primary deny reason for now
2. **No multi-violation tracking** - Cross-category cases logged as single category (highest severity)
3. **Interpersonal Safety Mode default** - Using this as baseline, Platform Content mode testing deferred
4. **No profanity toggle** - Including profanity in all audits for now, toggle deferred
5. **Keep audit data as-is** - Despite discovering biases mid-round, completing Round 1 for baseline data

### Why Proceed Despite Known Limitations?

**Rationale:** Round 1 is **base-level calibration** to detect fundamental problems before building additional features.

**The question we're answering:**
> "Are the LLMs and human auditor even remotely aligned on basic allow/deny decisions?"

**Using Gwet's AC1** (inter-rater reliability) to measure:
- Overall agreement rates between human, Gemini, Claude
- Category-specific agreement patterns
- Test case type difficulty (edge cases vs clear violations)

**If agreement is LOW (<0.6):**
- üö® Fundamental problems with prompts, categories, or LLM choice
- Need to fix core issues before adding multi-violation tracking, severity scoring, etc.
- Building advanced features on misaligned foundation = wasted effort

**If agreement is ACCEPTABLE (0.6-0.8):**
- ‚úÖ Core moderation logic is sound
- Safe to proceed with Round 2 improvements
- Can build confidence scores, severity tracking, multi-category support

**If agreement is HIGH (>0.8):**
- üéØ Excellent baseline, LLMs tracking human judgment well
- Focus Round 2 on edge cases and nuanced features
- Can move quickly to production-readiness

**Bottom line:** We need to know if we're building on solid ground or quicksand. Round 1 limitations are acceptable for this calibration goal. Once we confirm basic alignment, Round 2 can address the sophistication gaps.

**Cost of waiting:** Several weeks of development time to perfect Round 1, only to discover fundamental misalignment that requires starting over.

**Cost of proceeding:** Some noisy data, but enough signal to make high-level go/no-go decisions about the current approach.

---

## Round 2 (Planned)

**Goal:** Address Round 1 limitations and expand test coverage

### Planned Improvements

**Test Generation:**
- [ ] Regenerate context cases with speaker attribution
- [ ] Ensure balanced context distribution (40% condemns, 40% exonerates, 20% neutral/irrelevant)
- [ ] Add explicit temporal ordering to all context arrays
- [ ] Generate confession-specific test cases (past vs future tense)
- [ ] Add "Platform Content Standards Mode" test cases
- [ ] Generate ideation-specific test cases to test threat vs violence vs personal expression boundaries

**Audit Process:**
- [ ] All auditors review CATEGORY_DEFINITIONS.md before starting
- [ ] Implement category severity selection (primary + secondary categories)
- [ ] Track confidence scores for all auditors
- [ ] Add audit reasoning capture for disagreements
- [ ] Update LLM prompts to distinguish threats vs violence vs ideation:
  - "I will hurt you" = threats
  - "I fantasize about hurting people" = violence (ideation)
  - "I want to see violence" = allow/escalate (personal expression, mode-dependent)

**Methodology:**
- [ ] Pre-audit calibration session (sample 10 cases, discuss, align)
- [ ] Periodic check-ins for edge cases
- [ ] Post-audit review of high-disagreement cases

**Infrastructure:**
- [ ] Interactive menu-driven audit CLI (less --flags, more navigation)
- [ ] Audit set creation and management
- [ ] Better error logging (JSON parse vs content refusal)
- [ ] Rate limiting for API calls

### Questions to Answer in Round 2

1. Do confession cases have lower inter-rater reliability?
2. Does context speaker attribution improve agreement?
3. How does Platform Content mode change deny rates?
4. Are there category-specific bias patterns?
5. Which obfuscation techniques are most challenging?

---

## Round 3+ (Future)

**Potential areas:**
- Multi-language expansion (currently EN/JA heavy)
- Image moderation test cases
- Real-world production data sampling
- A/B testing different moderation philosophies
- Fine-tuning based on disagreement patterns

---

## Metrics Tracking

### Agreement Metrics by Round

| Round | Human-Gemini | Human-Claude | Human-GPT5.1 | Claude-Gemini | Notes |
|-------|--------------|--------------|--------------|---------------|-------|
| 1 | **92.9%** (AC1=0.905) | 84.9% (AC1=0.793) | 83.8% (AC1=0.778) | 91.0% (AC1=0.876) | Baseline complete |
| 2 | - | - | - | - | Planned |

**Round 1 Summary:**
- All pairs show Good to Excellent agreement (AC1 > 0.77)
- Highest: Human-Gemini (92.9%)
- Lowest: Human-GPT-5.1 (83.8%)
- Main disagreement: Profanity handling (LLMs more lenient)
- Gemini refused 2.7% of cases (17/636)

See `QA_REPORT_ROUND1.md` for full analysis.

### Label Verification Sampling (TODO)

**Purpose:** Catch generation errors before they corrupt analysis (learned from Apple paper incident).

Sample 50 cases stratified by type to verify generation labels are accurate:

- [ ] 10 `positive` (should violate) ‚Üí verify they actually look like clear violations
- [ ] 10 `negative` (should be clean) ‚Üí verify they're actually clean/innocent
- [ ] 10 `edge` ‚Üí verify they're genuinely ambiguous (not clear violations or clear allows)
- [ ] 10 `obfuscated` ‚Üí verify obfuscation is real and detectable
- [ ] 10 `cross` ‚Üí verify multiple categories actually apply

**Acceptance criteria:** If label error rate > 10%, regenerate affected batches.

**Note:** Generation labels (`category`, `type`) are metadata, NOT verified ground truth. Agreement metrics measure inter-rater reliability, not accuracy against GT.

---

### Ground Truth Strategy

**Two-tier approach:**

| Tier | Purpose | Owner | Size |
|------|---------|-------|------|
| **Library GT** | Regression testing, accuracy metrics | Us | ~100-200 curated cases |
| **API-level GT** | Platform-specific calibration | API users | Their responsibility |

**Library GT (TODO - Phase 2.10):**
- Small, high-quality curated set
- Expert-verified labels (human consensus, not model-generated)
- Clear violations and clear allows only (no edge cases)
- Used for: regression testing, accuracy reporting, model comparison
- Categories: All 11, balanced distribution
- Languages: EN primary, JA secondary

**What Library GT enables:**
```typescript
// Regression test
npm run test:accuracy
// Output: "Default config: 94% accuracy on curated GT (188/200 cases)"

// Model comparison  
npm run test:accuracy --provider=gemini
npm run test:accuracy --provider=openai
// Output: "Gemini: 91%, OpenAI: 93% on curated GT"
```

**API-level GT (documentation only):**
- Platforms define their own GT based on their policies
- We provide tooling to run accuracy tests against custom GT
- Example: Discord might allow more profanity than a kids' app

**Why both:**
- Library GT: "Does our default work?" (our responsibility)
- API GT: "Does it work for YOUR policy?" (their responsibility)

---

### Disagreement Patterns to Watch

- [ ] Confession cases (past tense violence)
- [ ] Context-dependent cases
- [ ] Cross-category cases
- [ ] Obfuscated content
- [ ] Cultural context (especially Japanese)
- [ ] Reclaimed language / in-group usage
- [ ] **Violent ideation vs threats** - LLMs conflate "I fantasize about X" with "I will do X"

### Interesting Cases to Review

**Case #73 (violence-positive-en-1764166355516-5) - Violent Ideation vs Threat:**
- **Text:** "I keep fantasizing about walking into my office with an AR-15 and watching my coworkers' heads explode one by one. Dave from accounting would be first..."
- **Why interesting:** Tests distinction between violent ideation/fantasy and direct threats
- **Key distinction:**
  - ‚ùå NOT a threat: "I fantasize about X" ‚â† "I will do X"
  - ‚úÖ IS violence: Graphic mass violence fantasy, specific victims
- **Audits:** 
  - Gemini: DENY ‚úÖ (reasoning: called it a "credible **threat**" - ‚ùå wrong category)
  - Claude: DENY ‚úÖ (reasoning: called it a "credible **threat** of violence" - ‚ùå wrong category)
  - Human: DENY ‚úÖ (correct reasoning: NOT a threat, but deny for graphic mass violence ideation)
- **Finding:** **Both LLMs misclassified category as "threats" when it should be "violence"**
  - Got the action right (deny)
  - Got the category wrong (threats vs violence)
  - Conflated "severe violent content" with "threats"
- **Implication for Round 2:** LLM prompts may need clearer guidance on threat vs violence vs ideation boundaries

---

## Changelog

| Date | Change | Impact |
|------|--------|--------|
| 2024-11-27 | Removed human audit metadata display | Fixed information leakage |
| 2024-11-27 | Removed Gemini category hint | Fixed LLM bias |
| 2024-11-27 | Generated context supplement (86 cases) | Partially addressed context bias |
| 2024-11-27 | Created CATEGORY_DEFINITIONS.md | Formalized all category boundaries |
| 2024-11-27 | Added confession vs threat framework | Clarified past vs future tense handling |
| 2024-11-27 | Documented context speaker attribution plan | Roadmap for Round 2 |
| 2024-11-28 | Fixed audit CLI back button | Better navigation |

---

## Notes

- This document is **living** - update as issues are discovered
- Each round should have a clear goal and success criteria
- Don't let perfect be the enemy of good - Round 1 data is still valuable despite limitations
- Document all decisions and rationale for future reference

